{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0712 15:07:55.511604 11156 deprecation.py:323] From <ipython-input-1-54036e27a456>:34: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0712 15:07:55.519049 11156 deprecation.py:506] From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0712 15:07:56.060681 11156 deprecation.py:323] From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0712 15:07:56.212920 11156 deprecation.py:506] From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17 ms ± 112 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "1.21 ms ± 57.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.27 ms ± 84.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.23 ms ± 21.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.21 ms ± 19.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.19 ms ± 14.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.29 ms ± 93.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.18 ms ± 11.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.2 ms ± 18.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.18 ms ± 19 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.17 ms ± 20.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.18 ms ± 15.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.18 ms ± 9.78 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.21 ms ± 19.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.22 ms ± 29.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.23 ms ± 6.88 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.21 ms ± 44.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.23 ms ± 67.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.4 ms ± 45.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import gym\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "\n",
    "def copy_net(name1, name2):\n",
    "    variables = tf.trainable_variables()\n",
    "    for var1 in variables:\n",
    "        if name2+\"/\" in var1.name:\n",
    "            trained_var = [var2 for var2 in tf.trainable_variables() if var2.op.name in str.replace(var1.name, name2+\"/\", name1+\"/\")][0]\n",
    "            value = sess.run(trained_var)\n",
    "            sess.run(tf.assign(var1, value))\n",
    "\n",
    "class QualityNet:\n",
    "    def __init__(self, **kwargs):\n",
    "        with tf.variable_scope(kwargs.get(\"net_name\")+\"/\"):\n",
    "            self.action_space_size = kwargs.get(\"action_space_size\", 2)\n",
    "            self.observation_space_size = kwargs.get(\"observation_space_size\", 2)\n",
    "            state_queue_size = kwargs.get(\"state_queue_size\", 4)\n",
    "\n",
    "            layers_config = kwargs.get(\"layers_config\", (self.action_space_size + state_queue_size*self.observation_space_size, 32)) \n",
    "\n",
    "            self.input_state = tf.placeholder(tf.float32, shape=[None, state_queue_size, self.action_space_size], name=\"input_state\")\n",
    "            self.flattened_state = tf.reshape(self.input_state, [-1, self.action_space_size * state_queue_size])\n",
    "            self.input_action = tf.placeholder(tf.int32, shape=[None], name=\"input_action\")\n",
    "            input_action_one_hot = tf.one_hot(self.input_action, depth=self.action_space_size)\n",
    "            \n",
    "            self.input_data = tf.concat([self.flattened_state, input_action_one_hot], 1)\n",
    "            \n",
    "            self.input_layer = tf.layers.dense(self.input_data, units=layers_config[0], activation='relu')\n",
    "            self.hidden_layer = tf.layers.dense(self.input_layer, units=layers_config[1], activation = 'relu')\n",
    "            self.output_layer = tf.layers.dense(self.hidden_layer, units=1)\n",
    "            \n",
    "            self.exp_value = tf.placeholder(tf.float32, name=\"exp_value\")\n",
    "            self.loss = tf.losses.mean_squared_error(self.exp_value, self.output_layer)\n",
    "            \n",
    "            optimizer = tf.train.RMSPropOptimizer(0.005)\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "        \n",
    "    def train(self, qvalues, batch): # TODO: refactor for batch\n",
    "        global sess\n",
    "        input_state = [batch[:, 0][i].queue for i in range(len(batch[:, 0]))]\n",
    "        input_action = batch[:, 1]\n",
    "        \n",
    "        _, loss = sess.run((self.train_op, self.loss), \n",
    "                           feed_dict = {\n",
    "                                self.input_state: input_state,\n",
    "                                self.input_action: input_action,\n",
    "                                self.exp_value: qvalues,\n",
    "                            })\n",
    "        return loss\n",
    "        \n",
    "    def feed_forward(self, states_list, action):\n",
    "        global sess\n",
    "        %timeit sess.run((self.output_layer), feed_dict = {self.input_state: states_list, self.input_action: action})\n",
    "        return 1\n",
    "    \n",
    "    def predict(self, state_queue):\n",
    "        rewards = np.array([])\n",
    "        for i in range(self.action_space_size+1):\n",
    "            reward = self.feed_forward([state_queue.queue], [i])\n",
    "            rewards = np.append(rewards, reward)\n",
    "        return rewards.max(), np.argmax(rewards)\n",
    "    \n",
    "class StateQueue():\n",
    "    queue = [np.array([0, 0]), np.array([0, 0]), np.array([0, 0]), np.array([0, 0])]\n",
    "    def __init__(self, size):\n",
    "        self.queue_size = size\n",
    "        \n",
    "    def update(self, state):\n",
    "        self.queue.insert(0, state)\n",
    "        if len(self.queue) > self.queue_size:\n",
    "            self.queue.pop()\n",
    "        return copy.copy(self)\n",
    "    \n",
    "    def last(self):\n",
    "        return self.queue[-1]\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.queue)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for s in self.queue:\n",
    "            yield s\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, env, **kwargs):\n",
    "        self.env = env\n",
    "        self.stochastic_action_likelihood = kwargs.get(\"stoch_act_ch\", 0.5)\n",
    "        self.stochastic_action_likelihood_d = kwargs.get(\"stoch_act_ch_d\", 0.01)\n",
    "        self.discount = kwargs.get(\"discount\", 0.9)\n",
    "        \n",
    "        prediction_net_params = kwargs.get(\"prediction_net_params\", {})\n",
    "        prediction_net_params[\"net_name\"] = \"prediction_net\"\n",
    "        self.prediction_net = QualityNet(**prediction_net_params )\n",
    "        \n",
    "        train_net_params = kwargs.get(\"train_net_params\", {})\n",
    "        train_net_params[\"net_name\"] = \"train_net\"\n",
    "        self.train_net = QualityNet(**train_net_params)\n",
    "    \n",
    "    def get_qvalues(self, batch):\n",
    "        qvals = list()\n",
    "        for b in batch:\n",
    "            q, _ = self.prediction_net.predict(b[3])\n",
    "            qvals.append(b[2] + self.discount * q)\n",
    "        return qvals\n",
    "    \n",
    "    def get_random_batch(self, history, batch_size):\n",
    "        return history[np.random.choice(history.shape[0], size=batch_size), :]\n",
    "    \n",
    "    def run(self, iterations=1000, batch_size=16, state_window=4, update_net_period=5, render_every=10):\n",
    "        history = np.array([])\n",
    "        \"\"\"\n",
    "            history[0] = queue of four last states\n",
    "            history[1] = action\n",
    "            history[2] = reward\n",
    "            history[3] = states queue with new state\n",
    "        \"\"\"\n",
    "        state_queue = StateQueue(state_window)\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            observation = self.env.reset()\n",
    "            state_queue.update(observation)\n",
    "            \n",
    "            for j in range(self.env.spec.max_episode_steps):                \n",
    "                if random.random() < self.stochastic_action_likelihood or state_queue.size() < state_window:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    _, action = self.prediction_net.predict(state_queue)\n",
    "\n",
    "                observation, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                new_history = (state_queue, action, reward, state_queue.update(observation))\n",
    "                history = np.append(history, new_history)\n",
    "                history = history.reshape(len(history) // len(new_history), len(new_history))\n",
    "                \n",
    "                random_batch = self.get_random_batch(history, batch_size)\n",
    "\n",
    "                qvalues = self.get_qvalues(random_batch)\n",
    "                self.train_net.train(qvalues, random_batch)\n",
    "                    \n",
    "                if i % render_every == 0:\n",
    "                    self.env.render()\n",
    "\n",
    "            if i % update_net_period == 0:\n",
    "                copy_net(\"train_net\", \"prediction_net\")\n",
    "\n",
    "            print(\"iteration {}\".format(i))    \n",
    "            self.stochastic_action_likelihood -= self.stochastic_action_likelihood_d\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "    ql = QLearning(env)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    ql.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1]] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [0], [0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
