{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hoxBEVcAmvQF"
   },
   "source": [
    "<p style=\"align: center;\"><img align=center src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" style=\"height:90px;\" width=500/></p>\n",
    "\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqKI3PvQqk-x"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Q6NgIGBmvQH"
   },
   "source": [
    "В этом ноутбке мы научимся писать свои свёрточные нейросети на фреймворке PyTorch, и протестируем их работу на датасетах MNIST и CIFAR10. \n",
    "\n",
    "**ВНИМАНИЕ:** Рассматривается ***задача классификации изображений***.\n",
    "\n",
    "(Подразумевается, что читатель уже знаком с многослойной нейроннной сетью).  \n",
    "\n",
    "***Свёрточная нейросеть (Convolutional Neural Network, CNN)*** - это многослойная нейросеть, имеющая в своей архитектуре помимо *полносвязных слоёв* (а иногда их может и не быть) ещё и **свёрточные слои (Conv Layers)** и **pooling-слои (Pool Layers)**.  \n",
    "\n",
    "Собственно, название такое эти сети получили потому, что в основе их работы лежит операция **свёртки**. \n",
    "\n",
    "\n",
    "Сразу же стоит сказать, что свёрточные нейросети **были придуманы прежде всего для задач, связанных с картинками**, следовательно, на вход они тоже \"ожидают\" картинку.\n",
    "\n",
    "Расмотрим их устройство более подробно:\n",
    "\n",
    "* Вот так выглядит неглубокая свёрточная нейросеть, имеющая такую архитектуру:  \n",
    "`Input -> Conv 5x5 -> Pool 2x2 -> Conv 5x5 -> Pool 2x2 -> FC -> Output`\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/269e3903f62eb2c4d13ac4c9ab979510010f8968/68747470733a2f2f7261772e6769746875622e636f6d2f746176677265656e2f6c616e647573655f636c617373696669636174696f6e2f6d61737465722f66696c652f636e6e2e706e673f7261773d74727565\" width=800, height=600>\n",
    "\n",
    "Свёрточные нейросети (обыкновенные, есть и намного более продвинутые) почти всегда строятся по следующему правилу:  \n",
    "\n",
    "`INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC`  \n",
    "\n",
    "то есть:  \n",
    "\n",
    "1). ***Входной слой*** (batch картинок `HxWxC`)  \n",
    "\n",
    "2). $M$ блоков (M $\\ge$ 0) из свёрток и pooling-ов, причём именно в том порядке, как в формуле выше. Все эти $M$ блоков вместе называют ***feature extractor*** свёрточной нейросети, потому что эта часть сети отвечает непосредственно за формирование новых, более сложных признаков, поверх тех, которые подаются (то есть, по аналогии с MLP, мы опять же переходим к новому признаковому пространству, однако здесь оно строится сложнее, чтем в обычных многослойных сетях, поскольку используется операция свёртки)  \n",
    "\n",
    "3). $K$ штук FullyConnected-слоёв (с активациями). Эту часть из $K$ FC-слоёв называют ***classificator***, поскольку эти слои отвечают непосредственно за предсказание нужно класса (сейчас рассматривается задача классификации изображений)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUh08RELmvQI"
   },
   "source": [
    "\n",
    "<h3 style=\"text-align: center;\"><b>Свёрточная нейросеть на PyTorch</b></h3>\n",
    "\n",
    "Ешё раз напомним про основные компоненты нейросети:\n",
    "\n",
    "- непосредственно, сама **архитектура** нейросети (сюда входят типы функций активации у каждого нейрона);\n",
    "- начальная **инициализация** весов каждого слоя;\n",
    "- метод **оптимизации** нейросети (сюда ещё входит метод изменения `learning_rate`);\n",
    "- размер **батчей** (`batch_size`);\n",
    "- количетсво итераций обучения (`num_epochs`);\n",
    "- **функция потерь** (`loss`);  \n",
    "- тип **регуляризации** нейросети (для каждого слоя можно свой);  \n",
    "\n",
    "То, что связано с ***данными и задачей***:  \n",
    "- само **качество** выборки (непротиворечивость, чистота, корректность постановки задачи);  \n",
    "- **размер** выборки;  \n",
    "\n",
    "Так как мы сейчас рассматриваем **архитектуру CNN**, то, помимо этих компонент, в свёрточной нейросети можно настроить следующие вещи:  \n",
    "\n",
    "- (в каждом ConvLayer) **размер фильтров (окна свёртки)** (`kernel_size`)\n",
    "- (в каждом ConvLayer) **количество фильтров** (`out_channels`)  \n",
    "- (в каждом ConvLayer) размер **шага окна свёртки (stride)** (`stride`)  \n",
    "- (в каждом ConvLayer) **тип padding'а** (`padding`)  \n",
    "\n",
    "\n",
    "- (в каждом PoolLayer) **размер окна pooling'a** (`kernel_size`)  \n",
    "- (в каждом PoolLayer) **шаг окна pooling'а** (`stride`)  \n",
    "- (в каждом PoolLayer) **тип pooling'а** (`pool_type`)  \n",
    "- (в каждом PoolLayer) **тип padding'а** (`padding`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aPy6-6kpmvQK"
   },
   "source": [
    "Какими их берут обычно -- будет показано в примере ниже. По крайней мере, можете стартовать с этих настроек, чтобы понять, какое качество \"из коробки\" будет у простой модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arl2FqAfmvQL"
   },
   "source": [
    "Посмотрим, как работает CNN на MNIST'е и на CIFAR'е:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ly4hPj7DmvQM"
   },
   "source": [
    "<img src=\"http://present5.com/presentation/20143288_415358496/image-8.jpg\" width=500, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94s_K_pAmvQN"
   },
   "source": [
    "**MNIST:** это набор из 70k картинок рукописных цифр от 0 до 9, написанных людьми, 60k из которых являются тренировочной выборкой (`train` dataset)), и ещё 10k выделены для тестирования модели (`test` dataset)."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 2,
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
<<<<<<< HEAD
    "collapsed": true,
=======
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
    "executionInfo": {
     "elapsed": 615,
     "status": "error",
     "timestamp": 1541754216636,
     "user": {
      "displayName": "Михаил Владимирович Макаров",
      "photoUrl": "",
      "userId": "00571021236711414160"
     },
     "user_tz": -180
    },
    "id": "O1tqZNdWmvQN",
    "outputId": "dd6c7d19-2350-4b53-de7f-588ca4532d61"
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fc0f6bd2228b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # для отрисовки картиночек\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kXXOvRS1mvQS"
   },
   "source": [
    "Скачаем и загрузим в `loader`'ы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmCjo8CjmvQT"
   },
   "source": [
    "**Обратите внимание на аргумент `batch_size`:** именно он будет отвечать за размер батча, который будет подаваться при оптимизации нейросети"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpjaLS99mvQV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
=======
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DpjaLS99mvQV"
   },
   "outputs": [],
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, \n",
    "                                      download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                     download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = tuple(str(i) for i in range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqtRMMqwmvQa"
   },
   "source": [
    "Сами данные лежат в полях `trainloader.dataset.train_data` и `testloader.dataset.test_data`:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxnQI-7FmvQc",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NxnQI-7FmvQc",
    "scrolled": true
   },
   "outputs": [],
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "source": [
    "trainloader.dataset.train_data.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-rt0UBALmvQf",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-rt0UBALmvQf",
    "scrolled": true
   },
   "outputs": [],
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "source": [
    "testloader.dataset.test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aj4QmKwDmvQk"
   },
   "source": [
    "Выведем первую картинку:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBAKbazhmvQm",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nBAKbazhmvQm",
    "scrolled": true
   },
   "outputs": [],
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "source": [
    "trainloader.dataset.train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7YqZYacmvQq"
   },
   "source": [
    "Посмотрим, как она выглядит:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 0,
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "SHxq21ydmvQq"
   },
   "outputs": [],
   "source": [
    "# преобразовать тензор в np.array\n",
    "numpy_img = trainloader.dataset.train_data[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lz-SfUuOmvQt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Lz-SfUuOmvQt"
   },
   "outputs": [],
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "source": [
    "numpy_img.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kAxc-OjwmvQy"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kAxc-OjwmvQy"
   },
   "outputs": [],
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "source": [
    "plt.imshow(numpy_img);"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jo1SInrpmvQ2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jo1SInrpmvQ2"
   },
   "outputs": [],
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "source": [
    "plt.imshow(numpy_img, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OW3JZteDmvQ7"
   },
   "source": [
    "Отрисовка заданной цифры:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VC-xcS7rmvQ7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VC-xcS7rmvQ7"
   },
   "outputs": [],
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "source": [
    "# случайный индекс от 0 до размера тренировочной выборки\n",
    "i = np.random.randint(low=0, high=60000)\n",
    "\n",
    "plt.imshow(trainloader.dataset.train_data[i].numpy(), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9DOvlk1mvRB"
   },
   "source": [
    "Как итерироваться по данным с помощью `loader'`а? Очень просто:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V73spLIAmvRD",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([3, 6, 0, 0])]\n"
     ]
    }
   ],
=======
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "V73spLIAmvRD",
    "scrolled": true
   },
   "outputs": [],
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "source": [
    "for data in trainloader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5gLVfQsLmvRI"
   },
   "source": [
    "То есть мы имеем дело с кусочками данных размера batch_size (в данном случае = 4), причём в каждом батче есть как объекты, так и ответы на них (то есть и $X$, и $y$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sftGrFWvmvRK"
   },
   "source": [
    "Теперь вернёмся к тому, что в PyTorch есть две \"парадигмы\" построения нейросетей -- `Functional` и `Seuquential`. Со второй мы уже хорошенько разобрались в предыдущих ноутбуках по нейросетям, теперь мы испольузем именно `Functional` парадигму, потому что при построении свёрточных сетей это намного удобнее:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 0,
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JjaimqEhmvRM"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # Functional"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
=======
   "execution_count": 0,
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JOY8j76OmvRP"
   },
   "outputs": [],
   "source": [
    "# ЗАМЕТЬТЕ: КЛАСС НАСЛЕДУЕТСЯ ОТ nn.Module\n",
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        # вызов конструктора предка\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "        # необходмо заранее знать, сколько каналов у картинки (сейчас = 1),\n",
    "        # которую будем подавать в сеть, больше ничего\n",
    "        # про входящие картинки знать не нужно\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 16, 120)  # !!!\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
<<<<<<< HEAD
    "        print(x.shape)\n",
=======
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 4 * 4 * 16)  # !!!\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDc3BlhPmvRR"
   },
   "source": [
    "**Важное примечание:** Вы можете заметить, что в строчках с `#!!!` есть не очень понятный сходу 4 `*` 4 `*` 16. Это -- размерность картинки перед FC-слоями (H x W x C), тут её приходиться высчитывать вручную (в Keras, например, `.Flatten()` всё делает за Вас). Однако есть один *лайфхак* -- просто сделайте в `forward()` `print(x.shape)` (закомментированная строка). Вы увидите размер `(batch_size, C, H, W)` -- нужно перемножить все, кроме первого (batch_size), это и будет первая размерность `Linear()`, и именно в C * H * W нужно \"развернуть\" x перед подачей в `Linear()`.  \n",
    "\n",
    "То есть нужно будет запустить цикл с обучением первый раз с `print()` и сделать после него `break`, посчитать размер, вписать его в нужные места и стереть `print()` и `break`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyg446camvRS"
   },
   "source": [
    "Код обучения слоя:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
=======
   "execution_count": 0,
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BtBorLrrmvRT"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ja5vFAvWmvRY",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c26dedddfaf4c9f8343cd84d452daf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121038f0933c48d2b769beb2d1f21ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "torch.Size([4, 1, 28, 28])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/runfme/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/runfme/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/runfme/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/runfme/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/runfme/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/runfme/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/runfme/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/runfme/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f0b3781e4579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
=======
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ja5vFAvWmvRY",
    "scrolled": true
   },
   "outputs": [],
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
   "source": [
    "# объявляем сеть\n",
    "net = SimpleConvNet()\n",
    "\n",
    "# выбираем функцию потерь\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# выбираем алгоритм оптимизации и learning_rate\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# итерируемся\n",
    "for epoch in tqdm_notebook(range(3)):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(tqdm_notebook(trainloader)):\n",
    "        # так получаем текущий батч\n",
    "        X_batch, y_batch = batch\n",
    "        \n",
    "        # обнуляем веса\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        y_pred = net(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # выведем текущий loss\n",
    "        running_loss += loss.item()\n",
    "        # выведем качество каждые 2000 батчей\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Обучение закончено')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJ8up_fZmvRc"
   },
   "source": [
    "Протестируем на всём тестовом датасете, используя метрику accuracy_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "RJc3Zea2mvRc"
   },
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        y_pred = net(images)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_83Wbv_UmvRf"
   },
   "source": [
    "Два свёрточных слоя побили многослойную нейросеть. Не магия ли?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ca7_nhaCmvRg"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPxtwt8DmvRi"
   },
   "source": [
    "### Задача 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2TQgBg3KmvRm"
   },
   "source": [
    "Протестируйте эту нейросеть на отдельных картинках из тестового датасета: напишите функцию, которая принимает индекс картинки в тестовом датасете, отрисовывает её, потом запускает на ней модель (нейросеть) и выводит результат предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UE1hoLa_mvRn"
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUKgvCXhmvRq"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cr4ftiAGmvRq"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>CIFAR10</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5ndS6LomvRt"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/soumith/ex/gh-pages/assets/cifar10.png\" width=500, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zxEQYnT9mvRt"
   },
   "source": [
    "**CIFAR10:** это набор из 60k картинок 32х32х3, 50k которых составляют обучающую выборку, и оставшиеся 10k - тестовую. Классов в этом датасете 10: `'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'`.\n",
    "\n",
    "Скачаем и загрузим в `loader`'ы:\n",
    "\n",
    "**Обратите внимание на аргумент `batch_size`:** именн он будет отвечать за размер батча, который будет подаваться при оптимизации нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KEH8UgeFmvRu"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-IxjpMXfmvRy"
   },
   "outputs": [],
   "source": [
    "# случайный индекс от 0 до размера тренировочной выборки\n",
    "i = np.random.randint(low=0, high=50000)\n",
    "\n",
    "plt.imshow(trainloader.dataset.train_data[i], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xAUH5D9tmvR0"
   },
   "source": [
    "То есть мы имеем дело с кусочками данных размера batch_size (в данном случае = 4), причём в каждом батче есть как объекты, так и ответы на них (то есть и $X$, и $y$).\n",
    "\n",
    "Данные готовы, мы даже на них посмотрели. **Однако учтите** - при подаче в нейросеть мы будем разворачивать картинку 32х32х3 в строку 1х(32`*`32`*`3) = 1х3072, то есть мы считаем пиксели (значения интенсивности в пикселях) за признаки нашего объекта (картинки).  \n",
    "\n",
    "К делу:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yIzpcJkZmvR2"
   },
   "source": [
    "### Задача 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FEzw5SbZmvR3"
   },
   "source": [
    "Напишите свою свёрточную нейросеть для предсказания на CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oCVhQM1mmvR3"
   },
   "outputs": [],
   "source": [
    "# ЗАМЕТЬТЕ: КЛАСС НАСЛЕДУЕТСЯ ОТ nn.Module\n",
    "class MyConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        # вызов конструктора предка\n",
    "        super(MyConvNet, self).__init__()\n",
    "        # Ваш код здесь\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ваш код здесь\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWwW8daBmvR7"
   },
   "source": [
    "Обучим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "e393qPEBmvR7"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EGOjA5ABmvR-"
   },
   "outputs": [],
   "source": [
    "# пример взят из официального туториала: \n",
    "# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "net = MyConvNet()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# итерируемся\n",
    "for epoch in tqdm_notebook(range(3)):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(tqdm_notebook(trainloader)):\n",
    "        # так получаем текущий батч\n",
    "        X_batch, y_batch = batch\n",
    "        \n",
    "        # РАЗВОРАЧИВАЕМ КАРТИНКУ В СТРОКУ\n",
    "        X_batch = X_batch.view(4, -1)\n",
    "\n",
    "        # обнуляем веса\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        y_pred = net(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # выведем текущий loss\n",
    "        running_loss += loss.item()\n",
    "        # выводем качество каждые 2000 батчей\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Обучение закончено')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u4doogQbmvSA"
   },
   "source": [
    "Посмотрим на accuracy на тестовом датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VEzyCJHXmvSA"
   },
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        y_pred = net(images.view(4, -1))\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4OF55kMmvSE"
   },
   "source": [
    "Как думаете, этого достаточно?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F__l8jHsmvSF"
   },
   "source": [
    "### Задача 3  \n",
    "\n",
    "Улучшите свёрточную нейросеть: поэкспериментируйте с архитектурой (количество слоёв, порядок слоёв), с гиперпараметрами слоёв (размеры kernel_size, размеры pooling'а, количество kernel'ов в свёрточном слое) и с гиперпараметрами, указанными в \"Компоненты нейросети\" (см. памятку выше)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "moepyKFemvSG"
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nwz2zvrSmvSJ"
   },
   "source": [
    "(Ожидаемый результат -- скорее всего, сходу Вам не удастся выжать из Вашей сетки больше, чем ~70% accuracy (в среднем по всем классам). Если это что-то в этом районе - Вы хорошо постарались)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BIIpG-8lmvSK"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Полезные ссылки</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxjnt47smvSO"
   },
   "source": [
    "1). *Примеры написания нейросетей на PyTorch (офийиальные туториалы) (на английском): https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#examples  \n",
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0OoUDBDmvSP"
   },
   "source": [
    "2). ***Один из самых подробных и полных курсов по deep learning на данный момент - это курс Стэнфордского Университета (он вообще сейчас один из лидеров в области ИИ, его выпускники работают в Google, Facebook, Amazon, Microsoft, в стартапах в Кремниевой долине):  http://cs231n.github.io/***  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfCLur8VmvSR"
   },
   "source": [
    "3). Практически исчерпывающая информация по основам свёрточных нейросетей (из cs231n) (на английском):  \n",
    "\n",
    "http://cs231n.github.io/convolutional-networks/  \n",
    "http://cs231n.github.io/understanding-cnn/  \n",
    "http://cs231n.github.io/transfer-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9cYDmcZxmvSR"
   },
   "source": [
    "4). Видео о Computer Vision от Andrej Karpathy: https://www.youtube.com/watch?v=u6aEYuemt0M"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]convnet_pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.8"
=======
   "version": "3.6.7"
>>>>>>> b78b123b973c5c08cbd4325a49bcd349ae10a993
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
