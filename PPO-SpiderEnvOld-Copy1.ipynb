{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: Не найден указанный модуль.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ac4b05198e5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmpi4py\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMPI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: Не найден указанный модуль."
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: Не найден указанный модуль.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1237a416f762>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcore\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlogx\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEpochLogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmpi_tf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMpiAdamOptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msync_all_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmpi_tools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmpi_fork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmpi_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproc_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmpi_statistics_scalar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_procs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Sirius\\logx.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mosp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0matexit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmpi_tools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mproc_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmpi_statistics_scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mserialization_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvert_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Sirius\\mpi_tools.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmpi4py\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMPI\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: Не найден указанный модуль."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import time\n",
    "import core as core\n",
    "from logx import EpochLogger\n",
    "from mpi_tf import MpiAdamOptimizer, sync_all_params\n",
    "from mpi_tools import mpi_fork, mpi_avg, proc_id, mpi_statistics_scalar, num_procs\n",
    "\n",
    "from SpiderEnv import SpiderEnv\n",
    "from SpiderEnv2 import SpiderEnv as SpiderEnv2\n",
    "from SpiderEnvMany import SpiderEnv as SpiderEnvMany\n",
    "from SpiderEnvMany2 import SpiderEnv as SpiderEnvMany2\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"SpiderEnvMany2\"\n",
    "env = SpiderEnvMany2()\n",
    "actor_critic = core.mlp_actor_critic\n",
    "seed = 0 \n",
    "steps_per_epoch = 4000\n",
    "epochs = 5000\n",
    "gamma = 0.99 \n",
    "clip_ratio = 0.2 \n",
    "pi_lr = 3e-4\n",
    "vf_lr = 1e-3\n",
    "train_pi_iters = 80 \n",
    "train_v_iters = 80\n",
    "lam = 0.97\n",
    "max_ep_len = 1000\n",
    "target_kl = 0.01\n",
    "\n",
    "\n",
    "save_freq = 100\n",
    "log_info_every_epoch = 10\n",
    "cpu = 1\n",
    "exp_name = \"ppo\"\n",
    "\n",
    "\n",
    "ac_kwargs=dict(hidden_sizes=[100, 50, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_utils import setup_logger_kwargs\n",
    "logger_kwargs = setup_logger_kwargs(exp_name, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpi_fork(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_data = {\n",
    "    \"pi_loss\": [],\n",
    "    \"actor_gradients_max_abs\": [],\n",
    "    \"actor_gradients_equal_zero\": [],\n",
    "    \"v_loss\": [],\n",
    "    \"critic_gradients_max_abs\": [],\n",
    "    \"critic_gradients_equal_zero\": [],\n",
    "    \"value_mean\": [],\n",
    "    \"rewards\": [],\n",
    "    \"adv_mean\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = core.discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = core.discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = mpi_statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        return [self.obs_buf, self.act_buf, self.adv_buf, \n",
    "                self.ret_buf, self.logp_buf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 20:59:27.009950 140397783583616 deprecation_wrapper.py:119] From /home/suriknik/Projects/sirius/mujoco/core.py:14: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Log dir /home/suriknik/Projects/sirius/data/ppo/ppo_s0 already exists! Storing info there anyway.\n",
      "\u001b[32;1mLogging data to /home/suriknik/Projects/sirius/data/ppo/ppo_s0/progress.txt\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 20:59:27.234605 140397783583616 deprecation_wrapper.py:119] From /home/suriknik/Projects/sirius/mujoco/core.py:98: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0723 20:59:27.237413 140397783583616 deprecation.py:323] From /home/suriknik/Projects/sirius/mujoco/core.py:31: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0723 20:59:27.249476 140397783583616 deprecation.py:506] From /home/suriknik/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0723 20:59:28.531372 140397783583616 deprecation_wrapper.py:119] From /home/suriknik/Projects/sirius/mujoco/core.py:78: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0723 20:59:28.543626 140397783583616 deprecation_wrapper.py:119] From /home/suriknik/Projects/sirius/mujoco/core.py:80: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "W0723 20:59:28.995457 140397783583616 deprecation_wrapper.py:119] From /home/suriknik/Projects/sirius/mujoco/core.py:35: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "W0723 20:59:29.021775 140397783583616 deprecation.py:323] From <ipython-input-7-0439632cbe48>:37: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\n",
      "Number of parameters: \t pi: 90111, \t v: 89651\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 20:59:29.759853 140397783583616 deprecation.py:323] From /home/suriknik/Projects/sirius/mujoco/mpi_tf.py:63: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "logger = EpochLogger(**logger_kwargs)\n",
    "# logger.save_config(locals())\n",
    "\n",
    "seed += 10000 * proc_id()\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "obs_dim = env.observation_space.shape\n",
    "act_dim = env.action_space.shape\n",
    "\n",
    "# Share information about action space with policy architecture\n",
    "ac_kwargs['action_space'] = env.action_space\n",
    "\n",
    "# # Inputs to computation graph\n",
    "x_ph, a_ph = core.placeholders_from_spaces(env.observation_space, env.action_space)\n",
    "adv_ph, ret_ph, logp_old_ph = core.placeholders(None, None, None)\n",
    "\n",
    "# Main outputs from computation graph\n",
    "pi, logp, logp_pi, v = actor_critic(x_ph, a_ph, **ac_kwargs)\n",
    "\n",
    "# Need all placeholders in *this* order later (to zip with data from buffer)\n",
    "all_phs = [x_ph, a_ph, adv_ph, ret_ph, logp_old_ph]\n",
    "\n",
    "# Every step, get: action, value, and logprob\n",
    "get_action_ops = [pi, v, logp_pi]\n",
    "\n",
    "# Experience buffer\n",
    "local_steps_per_epoch = int(steps_per_epoch / num_procs())\n",
    "buf = PPOBuffer(obs_dim, act_dim, local_steps_per_epoch, gamma, lam)\n",
    "\n",
    "# Count variables\n",
    "var_counts = tuple(core.count_vars(scope) for scope in ['pi', 'v'])\n",
    "logger.log('\\nNumber of parameters: \\t pi: %d, \\t v: %d\\n'%var_counts)\n",
    "\n",
    "# PPO objectives\n",
    "ratio = tf.exp(logp - logp_old_ph)          # pi(a|s) / pi_old(a|s)\n",
    "min_adv = tf.where(adv_ph>0, (1+clip_ratio)*adv_ph, (1-clip_ratio)*adv_ph)\n",
    "pi_loss = -tf.reduce_mean(tf.minimum(ratio * adv_ph, min_adv))\n",
    "v_loss = tf.reduce_mean((ret_ph - v)**2)\n",
    "\n",
    "# Info (useful to watch during learning)\n",
    "approx_kl = tf.reduce_mean(logp_old_ph - logp)      # a sample estimate for KL-divergence, easy to compute\n",
    "approx_ent = tf.reduce_mean(-logp)                  # a sample estimate for entropy, also easy to compute\n",
    "clipped = tf.logical_or(ratio > (1+clip_ratio), ratio < (1-clip_ratio))\n",
    "clipfrac = tf.reduce_mean(tf.cast(clipped, tf.float32))\n",
    "\n",
    "# Optimizers\n",
    "train_pi = MpiAdamOptimizer(learning_rate=pi_lr).minimize(pi_loss)\n",
    "train_v = MpiAdamOptimizer(learning_rate=vf_lr).minimize(v_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Sync params across processes\n",
    "sess.run(sync_all_params())\n",
    "\n",
    "# Setup model saving\n",
    "logger.setup_tf_saver(sess, inputs={'x': x_ph}, outputs={'pi': pi, 'v': v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver()\n",
    "# saver.restore(sess, '/home/sirius/Рабочий стол/models/\"Ant-v2\"-epoch-700.ckpt.data-00000-of-00001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    inputs = {k:v for k,v in zip(all_phs, buf.get())}\n",
    "    pi_l_old, v_l_old, ent = sess.run([pi_loss, v_loss, approx_ent], feed_dict=inputs)\n",
    "\n",
    "    # Training\n",
    "    for i in range(train_pi_iters):\n",
    "        _, kl = sess.run([train_pi, approx_kl], feed_dict=inputs)\n",
    "        kl = mpi_avg(kl)\n",
    "        if kl > 1.5 * target_kl:\n",
    "            logger.log('Early stopping at step %d due to reaching max kl.'%i)\n",
    "            break\n",
    "    logger.store(StopIter=i)\n",
    "    for _ in range(train_v_iters):\n",
    "        sess.run(train_v, feed_dict=inputs)\n",
    "\n",
    "    # Log changes from update\n",
    "    pi_l_new, v_l_new, kl, cf = sess.run([pi_loss, v_loss, approx_kl, clipfrac], feed_dict=inputs)\n",
    "    plots_data[\"pi_loss\"].append(pi_l_old)\n",
    "    plots_data[\"v_loss\"].append(v_l_old)\n",
    "    logger.store(LossPi=pi_l_old, LossV=v_l_old, \n",
    "                 KL=kl, Entropy=ent, ClipFrac=cf,\n",
    "                 DeltaLossPi=(pi_l_new - pi_l_old),\n",
    "                 DeltaLossV=(v_l_new - v_l_old))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7a6fdbac1b4a0799a9f5860e649e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 20:59:47.137800 140397783583616 deprecation.py:323] From /home/suriknik/Projects/sirius/mujoco/logx.py:226: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "W0723 20:59:47.139538 140397783583616 deprecation.py:323] From /home/suriknik/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: trajectory cut off by epoch at 105 steps.\n",
      "Model saved in path: ./models/PPO/SpiderEnvMany2/\"SE\"-epoch-0.ckpt\n",
      "\u001b[32;1mEarly stopping at step 1 due to reaching max kl.\u001b[0m\n",
      "---------------------------------------\n",
      "|             Epoch |               0 |\n",
      "|      AverageEpRet |            67.4 |\n",
      "|          StdEpRet |            1.43 |\n",
      "|          MaxEpRet |              70 |\n",
      "|          MinEpRet |            64.1 |\n",
      "|             EpLen |             130 |\n",
      "|      AverageVVals |          -0.415 |\n",
      "|          StdVVals |           0.273 |\n",
      "|          MaxVVals |           0.912 |\n",
      "|          MinVVals |          -0.936 |\n",
      "| TotalEnvInteracts |           4e+03 |\n",
      "|            LossPi |       -2.93e-08 |\n",
      "|             LossV |             691 |\n",
      "|       DeltaLossPi |          0.0521 |\n",
      "|        DeltaLossV |            -350 |\n",
      "|           Entropy |            16.6 |\n",
      "|                KL |           0.156 |\n",
      "|          ClipFrac |            0.66 |\n",
      "|          StopIter |               1 |\n",
      "|              Time |            26.6 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 105 steps.\n",
      "\u001b[32;1mEarly stopping at step 1 due to reaching max kl.\u001b[0m\n",
      "Warning: trajectory cut off by epoch at 105 steps.\n",
      "\u001b[32;1mEarly stopping at step 1 due to reaching max kl.\u001b[0m\n",
      "Warning: trajectory cut off by epoch at 102 steps.\n",
      "\u001b[32;1mEarly stopping at step 1 due to reaching max kl.\u001b[0m\n",
      "Warning: trajectory cut off by epoch at 100 steps.\n",
      "Warning: trajectory cut off by epoch at 105 steps.\n",
      "Warning: trajectory cut off by epoch at 102 steps.\n",
      "Warning: trajectory cut off by epoch at 100 steps.\n",
      "Warning: trajectory cut off by epoch at 98 steps.\n",
      "Warning: trajectory cut off by epoch at 100 steps.\n",
      "Warning: trajectory cut off by epoch at 100 steps.\n",
      "---------------------------------------\n",
      "|             Epoch |              10 |\n",
      "|      AverageEpRet |            68.3 |\n",
      "|          StdEpRet |            1.55 |\n",
      "|          MaxEpRet |            73.1 |\n",
      "|          MinEpRet |            63.6 |\n",
      "|             EpLen |             130 |\n",
      "|      AverageVVals |            15.6 |\n",
      "|          StdVVals |            4.02 |\n",
      "|          MaxVVals |            20.9 |\n",
      "|          MinVVals |            8.39 |\n",
      "| TotalEnvInteracts |         4.4e+04 |\n",
      "|            LossPi |       -1.58e-08 |\n",
      "|             LossV |             198 |\n",
      "|       DeltaLossPi |         -0.0332 |\n",
      "|        DeltaLossV |           -23.7 |\n",
      "|           Entropy |            16.5 |\n",
      "|                KL |         0.00456 |\n",
      "|          ClipFrac |           0.285 |\n",
      "|          StopIter |            55.6 |\n",
      "|              Time |             321 |\n",
      "---------------------------------------\n",
      "Warning: trajectory cut off by epoch at 102 steps.\n",
      "Warning: trajectory cut off by epoch at 100 steps.\n",
      "Warning: trajectory cut off by epoch at 100 steps.\n",
      "Warning: trajectory cut off by epoch at 103 steps.\n",
      "Warning: trajectory cut off by epoch at 101 steps.\n",
      "Warning: trajectory cut off by epoch at 101 steps.\n",
      "\u001b[32;1mEarly stopping at step 68 due to reaching max kl.\u001b[0m\n",
      "Warning: trajectory cut off by epoch at 105 steps.\n",
      "\u001b[32;1mEarly stopping at step 1 due to reaching max kl.\u001b[0m\n",
      "Warning: trajectory cut off by epoch at 104 steps.\n",
      "\u001b[32;1mEarly stopping at step 1 due to reaching max kl.\u001b[0m\n",
      "Warning: trajectory cut off by epoch at 102 steps.\n",
      "\u001b[32;1mEarly stopping at step 3 due to reaching max kl.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "\n",
    "# Main loop: collect experience in env and update/log each epoch\n",
    "for epoch in tqdm_notebook(range(epochs)):\n",
    "    for t in range(local_steps_per_epoch):\n",
    "        a, v_t, logp_t = sess.run(get_action_ops, feed_dict={x_ph: o.reshape(1,-1)})\n",
    "\n",
    "        # save and log\n",
    "        buf.store(o, a, r, v_t, logp_t)\n",
    "        logger.store(VVals=v_t)\n",
    "\n",
    "        o, r, d, _ = env.step(a[0])\n",
    "\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "\n",
    "        terminal = d or (ep_len == max_ep_len)\n",
    "        if terminal or (t==local_steps_per_epoch-1):\n",
    "            if not(terminal):\n",
    "                print('Warning: trajectory cut off by epoch at %d steps.'%ep_len)\n",
    "            # if trajectory didn't reach terminal state, bootstrap value target\n",
    "            last_val = r if d else sess.run(v, feed_dict={x_ph: o.reshape(1,-1)})\n",
    "            buf.finish_path(last_val)\n",
    "            if terminal:\n",
    "                # only save EpRet / EpLen if trajectory finished\n",
    "                plots_data[\"rewards\"].append(ep_ret)\n",
    "                logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "            o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "\n",
    "    # Save model\n",
    "    if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "        logger.save_state({'env': env}, None)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, f'./models/PPO/{env_name}/\"SE\"-epoch-{epoch}.ckpt')\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "    # Perform PPO update!\n",
    "    update()\n",
    "    \n",
    "    if epoch % log_info_every_epoch == 0:\n",
    "        # Log info about epoch\n",
    "        logger.log_tabular('Epoch', epoch)\n",
    "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('EpLen', average_only=True)\n",
    "        logger.log_tabular('VVals', with_min_and_max=True)\n",
    "        logger.log_tabular('TotalEnvInteracts', (epoch+1)*steps_per_epoch)\n",
    "        logger.log_tabular('LossPi', average_only=True)\n",
    "        logger.log_tabular('LossV', average_only=True)\n",
    "        logger.log_tabular('DeltaLossPi', average_only=True)\n",
    "        logger.log_tabular('DeltaLossV', average_only=True)\n",
    "        logger.log_tabular('Entropy', average_only=True)\n",
    "        logger.log_tabular('KL', average_only=True)\n",
    "        logger.log_tabular('ClipFrac', average_only=True)\n",
    "        logger.log_tabular('StopIter', average_only=True)\n",
    "        logger.log_tabular('Time', time.time()-start_time)\n",
    "        logger.dump_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average\n",
    "def smoothen_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "\n",
    "def plot(array, title='', axis=plt):\n",
    "    axis.title.set_text(title)\n",
    "    axis.plot(np.arange(len(array)), smoothen_curve(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#************ Plotting debug info ****************\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 9))\n",
    "plot(buf.rew_buf, \"Rewards\", axs[0, 0])\n",
    "plot(plots_data[\"v_loss\"], \"Critic loss\", axs[0, 1])\n",
    "plot(buf.adv_buf, \"Advantage mean\", axs[0, 2])\n",
    "plot(plots_data[\"pi_loss\"], \"Actor loss\", axs[1, 0])\n",
    "plot(buf.act_buf.max(axis=1), \"Max action\", axs[1,1])\n",
    "plt.suptitle(env_name, fontsize=18)\n",
    "plt.savefig(\"./models/PPO/{}/plots/reward({}).png\".format(env_name, int(np.array(buf.rew_buf[-10:]).mean())))\n",
    "#*************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SpiderEnvMany2()\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    try:\n",
    "        ac, _, _ = sess.run(get_action_ops, feed_dict={x_ph: obs.reshape(1,-1)})\n",
    "\n",
    "        obs, reward, done, _ = env.step(ac[0])\n",
    "        env.render()\n",
    "    except KeyboardInterrupt:\n",
    "        env.close()\n",
    "        break\n",
    "if done:\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
